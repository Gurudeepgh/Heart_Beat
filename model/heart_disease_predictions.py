# -*- coding: utf-8 -*-
"""heart-disease-predictions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QY386MPxhJRKekneNHO5Te0YQMGFL_2V

# Predicting Heart Disease using Machine Learning Model

This notebook looks into various python-based machine learning & Data Science libraries in an attempt to build a machine learning model capable of predicting whether or not someone has Heart Disease based on their medical attributes.

We're going to take following approach:
1. Problem Definition
2. Data
3. Evaluation
4. Features
5. Modelling
6. Experimenting

## 1. Problem Definition

In a statement,
> Given clinical parameters about a patient, can we predict whether or not they have heart disease?

## 2. Data

This is a multivariate type of dataset which means providing or involving a variety of separate mathematical or statistical variables, multivariate numerical data analysis. It is composed of 14 attributes which are age, sex, chest pain type, resting blood pressure, serum cholesterol, fasting blood sugar, resting electrocardiographic results, maximum heart rate achieved, exercise-induced angina, oldpeak â€” ST depression induced by exercise relative to rest, the slope of the peak exercise ST segment, number of major vessels and Thalassemia. This database includes 76 attributes, but all published studies relate to the use of a subset of 14 of them. The Cleveland database is the only one used by ML researchers to date. One of the major tasks on this dataset is to predict based on the given attributes of a patient that whether that particular person has heart disease or not and other is the experimental task to diagnose and find out various insights from this dataset which could help in understanding the problem more.

## 3. Evaluation

> if we can reach 95% accuracy at predicting whether or not someone has Heart Disease during the proof of concept, we'll pursue the project.

## 4. Features

This is where you'll get different information about each features in your Data.

**Create Data Dictionery **

* age: (Age of the patient in years)
* sex: (Male/Female)
* cp: chest pain type ([typical angina, atypical angina, non-anginal, asymptomatic])
* trestbps: resting blood pressure (resting blood pressure (in mm Hg on admission to the hospital))
* chol: (serum cholesterol in mg/dl)
* fbs: (if fasting blood sugar > 120 mg/dl)
* restecg: (resting electrocardiographic results)-- Values: [normal, stt abnormality, lv hypertrophy]
* thalach: maximum heart rate achieved
* exang: exercise-induced angina (True/ False)
* oldpeak: ST depression induced by exercise relative to rest
* slope: the slope of the peak exercise ST segment
* ca: number of major vessels (0-3) colored by fluoroscopy
* thal: [normal; fixed defect; reversible defect]
* num: the predicted attribute

# Preparing the tools

We're going to use pandas, numpy and matplotlib for data analysis & manipulation.
"""

# Commented out IPython magic to ensure Python compatibility.
# Import all the tools

# Regular EDA (Exploratory Data Analysis) & plotting libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# Models from scikit-learn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, log_loss
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC, NuSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
import warnings


# Model Evaluations

from sklearn.model_selection import train_test_split , cross_val_score , RandomizedSearchCV , GridSearchCV
from sklearn.metrics import confusion_matrix , classification_report , precision_score , recall_score , f1_score,roc_curve, RocCurveDisplay



"""### Load Data"""

df = pd.read_csv("/content/heart.csv")
df.shape

"""# Data Exploration Analysis"""

# viewing top 5 rows of Data
df.head()

# Viewing last 5 rows of Data
df.tail()

# Let's find out how many of each class there (1= Having Heart Disease, 0= Not Having Heart Disease)
df["target"].value_counts()

# Showing each class on bar plot
df["target"].value_counts().plot(kind="bar", color= ["salmon", "lightblue"]);

df.info()

# Are tere any missiing values?
df.isna().sum()

df.describe()

"""### Heart Disease Frequency according to sex"""

df.sex.value_counts()

# viewing data through gender
pd.crosstab(df.sex,df.target)

pd.crosstab(df.target,df.sex).plot(kind="bar", figsize=(10,6) , color=["salmon", "lightblue"]);
plt.title("Heart Disease Frequency according to sex")
plt.xlabel("0=No Heart Disease, 1=Heart Diease")
plt.ylabel("No of Persons")
plt.legend([ "Female","Male"])
plt.xticks(rotation=0);

"""### Age vs Max Heart Rate for Heart Disease"""

# Create another figure
plt.figure(figsize=(10,6))

# Scatter with positive examples
plt.scatter(df.age[df.target==1],
           df.thalach[df.target==1],
           c="salmon");

# Scatter with negative examples
plt.scatter(df.age[df.target==0],
           df.thalach[df.target==0],
           c="black");

# add some helpful title
plt.title("Heart Disease in function of Age & Max Heart Rate")
plt.xlabel("Age")
plt.ylabel("Max Heart Rate")
plt.legend(["Disease", "No Disease"]);



# Check the distribution of age column with histogram
df.age.plot.hist();

pd.crosstab(df.cp, df.target)

pd.crosstab(df.cp, df.target).plot(kind="bar", figsize=(10,6), color=["salmon", "lightblue"])
plt.title("Heart Disease Frequency per chest pain type")
plt.xlabel("Chest Pain Type")
plt.ylabel("Amount")
plt.legend(["No Disease", "Disease"])
plt.xticks(rotation=0);

# Showing correlation btw attributes
df.corr()

# visualizing correlation in heatmap
corr_matrix = df.corr()
fig, ax =plt.subplots(figsize=(15,7))
ax = sns.heatmap(corr_matrix,
                annot=True,
                linewidths=0.5,
                fmt=".2f",
                cmap="YlGnBu");

"""# Modeling"""

# splitting data into x & y
x = df.drop("target",axis=1)
y = df["target"]

# splitting data into train & test sets
np.random.seed(42)
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3)

# Fits & Evaluates given Machine Learning Models
models = {"Logistic Regression": LogisticRegression(solver='lbfgs', max_iter=3000),
         "KNN": KNeighborsClassifier(),
         "Random Forest": RandomForestClassifier(n_estimators=100, criterion="entropy",max_depth=7, min_samples_leaf=2, max_features="sqrt",random_state=2),
    "SVC":SVC(kernel="rbf", C=0.025, probability=True),
    "NSVC":NuSVC(probability=True),
    "DTC":DecisionTreeClassifier(),
    "ABC":AdaBoostClassifier(),
    "GBC":GradientBoostingClassifier(),
    "GNB":GaussianNB(),
    "LDA":LinearDiscriminantAnalysis(),
    "QDA":QuadraticDiscriminantAnalysis()
         }
def fit_and_score(models, x_train, x_test, y_train, y_test):
    """
    Fits & Evaluates given Machine Learning Models
    models= A dict of Different Scikit Learn Models
    x_train= Training Data with no labels
    x_test= Test Data with no labels
    y_train= Training Labels
    y_test= Test Labels
    """
    # Set random seed
    np.random.seed(42)
    #Make a dictionary to keep model scores
    model_scores={}
    # Creating For loop to train Models
    for name,model in models.items():
        # fit the model to data
        model.fit(x_train, y_train)
        # return scores and store in model_scores dict
        model_scores[name]= model.score(x_test,y_test)
    return model_scores

# Fitting & Evaluating given Machine Learning Models
model_scores=fit_and_score(models, x_train,x_test,y_train,y_test)

model_scores

model_compare = pd.DataFrame(model_scores, index=["Accuracy"])
model_compare.T.plot.bar(legend=False, yticks=np.arange(0.1,1.0,0.08));

"""# Improving Models

### 1. Hyperparameter Tuning (by hand)
"""

# Create Hyperparameter grid for RandomForestClassifier
rf_grid = {"n_estimators": np.arange(10,1000, 50),
          "max_depth":[None, 3,5,7,10,15],
          "min_samples_split":np.arange(2,20,2),
          "min_samples_leaf":np.arange(1,20,2)}

# Tune RandomForestClassifier
np.random.seed(42)
# Setup random hyperparameter search for RandomForestClassifier
rf_log_reg= RandomizedSearchCV(RandomForestClassifier(), param_distributions= rf_grid, cv=5,
                               verbose=True)
# Fit Random Hyperparameter search model for andomForestClassifier
rf_log_reg.fit(x_train,y_train)

rf_log_reg.best_params_

rf_log_reg.score(x_test,y_test)

y_preds = rf_log_reg.predict(x_test)

y_preds

RocCurveDisplay.from_estimator(rf_log_reg, x_test, y_test);

sns.set(font_scale=1.5)
def plot_conf_mat(y_test,y_preds):
    """
    Plots a nice looking confusion matrix on seaborn heatmap
    """
    fig, ax = plt.subplots(figsize=(3,3))
    ax = sns.heatmap(confusion_matrix(y_test,y_preds),fmt = ".0f", annot=True, cbar=False)
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
plot_conf_mat(y_test, y_preds)

print(classification_report(y_test,y_preds))

"""# Calculate evaluation metrics using cross-validation

we're going to calculate accuracy, precision,recall, , f1-score of our model using cross-validation and to do so we'll be using `cross_val_score()`
"""

# check best hyperparameters
rf_log_reg.best_params_

#create a new classifier using best params
clf = RandomForestClassifier()

# Cross-Validated Accuracy
cv_acc = cross_val_score(clf, x,y,
                        cv=5,
                        scoring="accuracy")
cv_acc
cv_acc =  np.mean(cv_acc)
cv_acc

# Cross-Validated Precision
cv_precision = cross_val_score(clf, x,y,
                        cv=5,
                        scoring="precision")
cv_precision
cv_precision =  np.mean(cv_precision)
cv_precision

# Cross-Validated Recall
cv_recall = cross_val_score(clf, x,y,
                        cv=5,
                        scoring="recall")
cv_recall
cv_recall =  np.mean(cv_recall)
cv_recall

# Cross-Validated F1-score
cv_f1 = cross_val_score(clf, x,y,
                        cv=5,
                        scoring="f1")
cv_f1
cv_f1 =  np.mean(cv_f1)
cv_f1

# Visualize Cross_validated metrics
cv_metrics = pd.DataFrame({"Accuracy":cv_acc,
                          "Precision": cv_precision,
                          "Recall":cv_recall,
                          "F1 Score":cv_f1}, index=[0])
cv_metrics.T.plot.bar(title="Cross Validated Classification Metrics",
                     legend=False);

"""# Feature Importance
Feature importance is another as asking"Which features contributed most to te outcomes of the model& how did they contribute?"

Finding feature importance is different for each machine learning model.
let's find out feature importance for our logistic regression model

"""

df.head()

# Fit an instance of LogisticRegression
rf_log_reg.best_params_
clf = RandomForestClassifier()
clf.fit(x_train, y_train)

# Check Coef_
clf.feature_importances_

df.head()

# Match coef's of feature to column
feature_dict= dict(zip(df.columns, list(clf.feature_importances_)))
feature_dict

# Visualize feature importance
feature_df = pd.DataFrame(feature_dict,index=[0])
feature_df.T.plot.bar(title="Feature Importance", legend=False);

import pickle

with open('heart_predict.pkl','wb') as file:
   pickle.dump(rf_log_reg, file)